{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b866fd6b-fab9-4a7c-a359-cfa32f5df8a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Securing MULTIMODAL biometrics using FHE\\\\PAPER2 LATEST'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f8541d6-a03d-4b20-80cc-7ed85a28321e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: torchvision in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (0.17.2)\n",
      "Requirement already satisfied: insightface in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (0.7.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from torch) (2.8.8)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from torch) (2024.6.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from torchvision) (1.24.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: onnx in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from insightface) (1.11.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from insightface) (4.67.0)\n",
      "Requirement already satisfied: requests in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from insightface) (2.32.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from insightface) (3.4.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from insightface) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from insightface) (1.3.2)\n",
      "Requirement already satisfied: scikit-image in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from insightface) (0.21.0)\n",
      "Requirement already satisfied: easydict in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from insightface) (1.13)\n",
      "Requirement already satisfied: cython in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from insightface) (3.0.11)\n",
      "Requirement already satisfied: albumentations in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from insightface) (1.3.1)\n",
      "Requirement already satisfied: prettytable in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from insightface) (3.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from albumentations->insightface) (6.0)\n",
      "Requirement already satisfied: qudida>=0.0.4 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from albumentations->insightface) (0.0.4)\n",
      "Requirement already satisfied: opencv-python-headless>=4.1.1 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from albumentations->insightface) (4.10.0.84)\n",
      "Requirement already satisfied: imageio>=2.27 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from scikit-image->insightface) (2.34.2)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from scikit-image->insightface) (2023.7.10)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from scikit-image->insightface) (1.4.1)\n",
      "Requirement already satisfied: packaging>=21 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from scikit-image->insightface) (24.1)\n",
      "Requirement already satisfied: lazy_loader>=0.2 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from scikit-image->insightface) (0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from matplotlib->insightface) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from matplotlib->insightface) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from matplotlib->insightface) (3.1.2)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from onnx->insightface) (3.20.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from prettytable->insightface) (0.2.13)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from requests->insightface) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from requests->insightface) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from requests->insightface) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from requests->insightface) (2024.6.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from scikit-learn->insightface) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from scikit-learn->insightface) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from tqdm->insightface) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision insightface pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b62233c2-6e92-4836-bd49-da03497d2f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2+cpu\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c77f7b22-9f24-442f-a041-39a5d9fa7110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.19\n"
     ]
    }
   ],
   "source": [
    "!python --version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92c72994-8a58-4016-8d97-682fbe2a81e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: insightface==0.7.3 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (0.7.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from insightface==0.7.3) (1.24.3)\n",
      "Requirement already satisfied: onnx in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from insightface==0.7.3) (1.11.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from insightface==0.7.3) (4.67.0)\n",
      "Requirement already satisfied: requests in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from insightface==0.7.3) (2.32.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from insightface==0.7.3) (3.4.3)\n",
      "Requirement already satisfied: Pillow in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from insightface==0.7.3) (10.2.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from insightface==0.7.3) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from insightface==0.7.3) (1.3.2)\n",
      "Requirement already satisfied: scikit-image in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from insightface==0.7.3) (0.21.0)\n",
      "Requirement already satisfied: easydict in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from insightface==0.7.3) (1.13)\n",
      "Requirement already satisfied: cython in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from insightface==0.7.3) (3.0.11)\n",
      "Requirement already satisfied: albumentations in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from insightface==0.7.3) (1.3.1)\n",
      "Requirement already satisfied: prettytable in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from insightface==0.7.3) (3.11.0)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from albumentations->insightface==0.7.3) (6.0)\n",
      "Requirement already satisfied: qudida>=0.0.4 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from albumentations->insightface==0.7.3) (0.0.4)\n",
      "Requirement already satisfied: opencv-python-headless>=4.1.1 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from albumentations->insightface==0.7.3) (4.10.0.84)\n",
      "Requirement already satisfied: networkx>=2.8 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from scikit-image->insightface==0.7.3) (2.8.8)\n",
      "Requirement already satisfied: imageio>=2.27 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from scikit-image->insightface==0.7.3) (2.34.2)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from scikit-image->insightface==0.7.3) (2023.7.10)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from scikit-image->insightface==0.7.3) (1.4.1)\n",
      "Requirement already satisfied: packaging>=21 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from scikit-image->insightface==0.7.3) (24.1)\n",
      "Requirement already satisfied: lazy_loader>=0.2 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from scikit-image->insightface==0.7.3) (0.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from matplotlib->insightface==0.7.3) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from matplotlib->insightface==0.7.3) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from matplotlib->insightface==0.7.3) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from matplotlib->insightface==0.7.3) (2.9.0)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from onnx->insightface==0.7.3) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from onnx->insightface==0.7.3) (4.12.2)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from prettytable->insightface==0.7.3) (0.2.13)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from requests->insightface==0.7.3) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from requests->insightface==0.7.3) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from requests->insightface==0.7.3) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from requests->insightface==0.7.3) (2024.6.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from scikit-learn->insightface==0.7.3) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from scikit-learn->insightface==0.7.3) (3.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from tqdm->insightface==0.7.3) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->insightface==0.7.3) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install insightface==0.7.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d9776ce-e840-4bea-afd7-8ef498ed49b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnx==1.11.0 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (1.11.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from onnx==1.11.0) (1.24.3)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from onnx==1.11.0) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from onnx==1.11.0) (4.12.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install onnx==1.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52a9aecc-8700-4c36-a4ba-c617a2008327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnxruntime==1.10.0 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (1.10.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from onnxruntime==1.10.0) (1.24.3)\n",
      "Requirement already satisfied: protobuf in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from onnxruntime==1.10.0) (3.20.3)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from onnxruntime==1.10.0) (24.3.25)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install onnxruntime==1.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91a78579-d770-47bd-8b1d-eb050ad16f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: torchvision in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (0.17.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (1.24.3)\n",
      "Requirement already satisfied: pillow in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (10.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from torch) (2.8.8)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from torch) (2024.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\srmap\\anaconda3\\envs\\newenv\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision numpy pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "2833cc45-987f-42cf-afdb-72dd8721affe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Securing MULTIMODAL biometrics using FHE\\\\PAPER2 LATEST'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef41a36-6292-4550-b921-f6709ef4ade5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from insightface.app import FaceAnalysis\n",
    "\n",
    "# Initialize the ArcFace model using InsightFace\n",
    "app = FaceAnalysis(allowed_modules=['detection', 'recognition'])\n",
    "app.prepare(ctx_id=0, det_size=(320, 320))  # You can change det_size=(320, 320) for small images\n",
    "\n",
    "# Directory paths\n",
    "# DATA_DIR = r'D:\\paper2\\NEWCODE\\ORL_Face_dataset_2image' \n",
    "# output_csv_path = 'D:/Securing MULTIMODAL biometrics using FHE/PAPER2 LATEST/Features/ORL_features_arcface_2img.csv'\n",
    "\n",
    "DATA_DIR = r'/dataset-path'\n",
    "output_csv_path = 'Features.csv'\n",
    "# Initialize a list to store features and image names\n",
    "features_list = []\n",
    "\n",
    "# Process each image in the dataset\n",
    "for class_dir in sorted(os.listdir(DATA_DIR)):\n",
    "    class_path = os.path.join(DATA_DIR, class_dir)\n",
    "    if os.path.isdir(class_path):\n",
    "        for img_name in sorted(os.listdir(class_path)):\n",
    "            if img_name.lower().endswith('.jpg'):\n",
    "                img_path = os.path.join(class_path, img_name)\n",
    "                print(f\"Processing {img_path}...\")\n",
    "\n",
    "                # Load image and convert to RGB NumPy array\n",
    "                try:\n",
    "                    img = Image.open(img_path).convert('RGB')\n",
    "                    img_np = np.array(img)\n",
    "\n",
    "                    # Extract features using ArcFace\n",
    "                    faces = app.get(img_np)\n",
    "                    if len(faces) > 0:\n",
    "                        feature_vector = faces[0].embedding.flatten()\n",
    "\n",
    "                        # Construct image identifier and save features\n",
    "                        img_identifier = f\"{class_dir}/{img_name}\"\n",
    "                        features_list.append([img_identifier] + feature_vector.tolist())\n",
    "                    else:\n",
    "                        print(f\"No face detected in {img_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {img_path}: {e}\")\n",
    "\n",
    "# Generate column names for the header\n",
    "if features_list:\n",
    "    num_features = len(features_list[0]) - 1  # Subtract 1 for 'Image Name' column\n",
    "    column_names = ['Image Name'] + [f'f{i+1}' for i in range(num_features)]\n",
    "\n",
    "    # Convert to a DataFrame and save to CSV\n",
    "    df = pd.DataFrame(features_list, columns=column_names)\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"\\nFeature extraction completed. Features saved to:\\n{output_csv_path}\")\n",
    "else:\n",
    "    print(\"No features were extracted. Please check your input images.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16dd1fa-7ed2-4336-8507-5c9565e21ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.linalg import eigh\n",
    "\n",
    "# Load the face features from the CSV file\n",
    "csv_path = 'features.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Extract the feature vectors (ignoring the first column with image names)\n",
    "X = df.iloc[:, 1:].values\n",
    "\n",
    "# LPP function\n",
    "def compute_weight_matrix(X, epsilon):\n",
    "    distances = squareform(pdist(X.T, 'euclidean'))\n",
    "    W = np.zeros_like(distances)\n",
    "    W[distances <= epsilon] = 1\n",
    "    return W\n",
    "\n",
    "def compute_laplacian_matrix(W):\n",
    "    D = np.diag(np.sum(W, axis=1))\n",
    "    L = D - W\n",
    "    return L\n",
    "\n",
    "def lpp(X, num_components, epsilon):\n",
    "    W = compute_weight_matrix(X, epsilon)\n",
    "    L = compute_laplacian_matrix(W)\n",
    "    D = np.diag(np.sum(W, axis=1))\n",
    "    \n",
    "    # Eigen decomposition of the generalized eigenvalue problem\n",
    "    eigvals, eigvecs = eigh(L, D)\n",
    "    \n",
    "    # Sort the eigenvectors by eigenvalues\n",
    "    sorted_indices = np.argsort(eigvals)\n",
    "    eigvecs = eigvecs[:, sorted_indices]\n",
    "    \n",
    "    # Select the top `num_components` eigenvectors\n",
    "    V = eigvecs[:, :num_components]\n",
    "    \n",
    "    # Project the original data using the selected eigenvectors\n",
    "    X_reduced = np.dot(X, V)  \n",
    "    return X_reduced\n",
    "\n",
    "# Parameters for LPP\n",
    "num_components = 256  # Reduce to 256 dimensions\n",
    "epsilon = 0.5        \n",
    "\n",
    "# Apply LPP\n",
    "X_reduced = lpp(X, num_components, epsilon)\n",
    "\n",
    "# Create a DataFrame to save the reduced features\n",
    "reduced_df = pd.DataFrame(X_reduced, columns=[f'f{i+1}' for i in range(num_components)])\n",
    "\n",
    "# Insert the image names back into the DataFrame\n",
    "reduced_df.insert(0, 'Image Name', df['Image Name'])\n",
    "\n",
    "# Save the reduced features to a new CSV file\n",
    "output_path = 'features_dim.csv'\n",
    "reduced_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"LPP feature reduction completed and saved to:\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e28a7015-a295-48e3-b01c-1d4dc444e08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Genuine and Impostor scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ae9929-64fb-470b-bf4c-c134a00c9573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import euclidean\n",
    "from itertools import combinations\n",
    "\n",
    "# Load the extracted features\n",
    "def load_features(csv_filename):\n",
    "    features = {}\n",
    "    with open(csv_filename, 'r') as csvfile:\n",
    "        csv_reader = csv.reader(csvfile)\n",
    "        headers = next(csv_reader)  # Skip header\n",
    "        for row in csv_reader:\n",
    "            image_name = row[0]\n",
    "            feature_vector = np.array(row[1:], dtype=float)\n",
    "            features[image_name] = feature_vector\n",
    "    return features\n",
    "\n",
    "# Calculate Euclidean distance and classify pairs\n",
    "def classify_pairs(features):\n",
    "    genuine_pairs = []\n",
    "    impostor_pairs = []\n",
    "    \n",
    "    all_pairs = list(combinations(features.keys(), 2))\n",
    "    \n",
    "    for (image1, image2) in all_pairs:\n",
    "        class1 = image1.split('/')[0]\n",
    "        class2 = image2.split('/')[0]\n",
    "        dist = euclidean(features[image1], features[image2])\n",
    "        \n",
    "        if class1 == class2:\n",
    "            genuine_pairs.append((image1, image2, dist))\n",
    "        else:\n",
    "            impostor_pairs.append((image1, image2, dist))\n",
    "    \n",
    "    return genuine_pairs, impostor_pairs\n",
    "\n",
    "# Save pairs to CSV files\n",
    "def save_pairs_to_csv(pairs, filename):\n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow(['Image Pair', 'Distance'])\n",
    "        for (image1, image2, dist) in pairs:\n",
    "            csv_writer.writerow([f\"{image1} - {image2}\", dist])\n",
    "\n",
    "features_filename = 'features-dim.csv'\n",
    "\n",
    "\n",
    "genuine_output_filename = 'genuine-arcface.csv'\n",
    "impostor_output_filename = 'impostor-arcface.csv'\n",
    "\n",
    "# Load features\n",
    "features = load_features(features_filename)\n",
    "\n",
    "# Classify pairs\n",
    "genuine_pairs, impostor_pairs = classify_pairs(features)\n",
    "\n",
    "# Save pairs to CSV files\n",
    "save_pairs_to_csv(genuine_pairs, genuine_output_filename)\n",
    "save_pairs_to_csv(impostor_pairs, impostor_output_filename)\n",
    "\n",
    "# Print the count of genuine and impostor matches\n",
    "print(f\"Number of genuine matches: {len(genuine_pairs)}\")\n",
    "print(f\"Number of impostor matches: {len(impostor_pairs)}\")\n",
    "\n",
    "print(\"Genuine and impostor matches calculation and saving complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b2cf0e5f-fb2b-4743-a768-c39805602649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find and plot FAR , FRR and EER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da7fec3-97fd-460d-806d-087f41443882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load distances from CSV files\n",
    "def load_distances(csv_filename):\n",
    "    distances = []\n",
    "    with open(csv_filename, 'r') as csvfile:\n",
    "        csv_reader = csv.reader(csvfile)\n",
    "        next(csv_reader)  # Skip header\n",
    "        for row in csv_reader:\n",
    "            dist = float(row[1])\n",
    "            distances.append(dist)\n",
    "    return distances\n",
    "\n",
    "# Calculate FAR, FRR, and EER\n",
    "def calculate_far_frr_eer(genuine_distances, impostor_distances):\n",
    "    genuine_distances = np.array(genuine_distances)\n",
    "    impostor_distances = np.array(impostor_distances)\n",
    "    \n",
    "    min_threshold = min(np.min(genuine_distances), np.min(impostor_distances))\n",
    "    max_threshold = max(np.max(genuine_distances), np.max(impostor_distances))\n",
    "    \n",
    "    thresholds = np.linspace(min_threshold, max_threshold, num=1000)\n",
    "    \n",
    "    frr = np.zeros(len(thresholds))\n",
    "    far = np.zeros(len(thresholds))\n",
    "\n",
    "    for i, threshold in enumerate(thresholds):\n",
    "        frr[i] = np.sum(genuine_distances > threshold) / len(genuine_distances)\n",
    "        far[i] = np.sum(impostor_distances <= threshold) / len(impostor_distances)\n",
    "        \n",
    "    eer_index = np.abs(frr - far).argmin()\n",
    "    eer = np.mean((frr[eer_index], far[eer_index]))\n",
    "    \n",
    "    return far, frr, eer, thresholds[eer_index], thresholds\n",
    "\n",
    "# File paths\n",
    "genuine_output_filename = 'genuine-arcface-dim.csv'\n",
    "impostor_output_filename = 'genuine-arcface-dim.csv'\n",
    "\n",
    "# Load distances from CSV files\n",
    "genuine_distances = load_distances(genuine_output_filename)\n",
    "impostor_distances = load_distances(impostor_output_filename)\n",
    "\n",
    "# Calculate FAR, FRR, and EER\n",
    "far, frr, eer, eer_threshold, thresholds = calculate_far_frr_eer(genuine_distances, impostor_distances)\n",
    "\n",
    "# Print results\n",
    "print(f\"EER: {eer}\")\n",
    "print(f\"Threshold at EER: {eer_threshold}\")\n",
    "\n",
    "# Plot FAR and FRR\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, far, label='FAR', color='red')\n",
    "plt.plot(thresholds, frr, label='FRR', color='blue')\n",
    "plt.axvline(x=eer_threshold, linestyle='--', color='green', label=f'EER Threshold: {eer_threshold:.4f}')\n",
    "plt.axhline(y=eer, linestyle='--', color='purple', label=f'EER: {eer:.4f}')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Rate')\n",
    "plt.title('FAR and FRR vs. Threshold')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Save the plot\n",
    "output_plot_filename = 'FAR-FRR.png'\n",
    "plt.savefig(output_plot_filename)\n",
    "\n",
    "print(f\"Plot saved as {output_plot_filename}\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6e40ac8e-d55f-49f6-ad63-d58f06eca6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives (TP): 41\n",
      "True Negatives (TN): 3280\n",
      "False Positives (FP): 0\n",
      "False Negatives (FN): 0\n",
      "Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "def calculate_confusion_matrix(genuine_distances, impostor_distances, threshold):\n",
    "    # Convert lists to NumPy arrays\n",
    "    genuine_distances = np.array(genuine_distances)\n",
    "    impostor_distances = np.array(impostor_distances)\n",
    "    \n",
    "    tp = np.sum(genuine_distances <= threshold)  # True Positives\n",
    "    fn = np.sum(genuine_distances > threshold)   # False Negatives\n",
    "    tn = np.sum(impostor_distances > threshold)  # True Negatives\n",
    "    fp = np.sum(impostor_distances <= threshold) # False Positives\n",
    "    \n",
    "    return tp, tn, fp, fn\n",
    "\n",
    "def calculate_accuracy(tp, tn, fp, fn):\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    return accuracy\n",
    "\n",
    "# Assuming you've already calculated the threshold at EER using the existing code\n",
    "eer_threshold = 15.42\n",
    " \n",
    "\n",
    "# Call the confusion matrix function\n",
    "tp, tn, fp, fn = calculate_confusion_matrix(genuine_distances, impostor_distances, eer_threshold)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = calculate_accuracy(tp, tn, fp, fn)\n",
    "\n",
    "# Print results\n",
    "print(f\"True Positives (TP): {tp}\")\n",
    "print(f\"True Negatives (TN): {tn}\")\n",
    "print(f\"False Positives (FP): {fp}\")\n",
    "print(f\"False Negatives (FN): {fn}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "63aec145-a805-40c9-90db-d047035398a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives (TP): 41\n",
      "True Negatives (TN): 3280\n",
      "False Positives (FP): 0\n",
      "False Negatives (FN): 0\n",
      "Accuracy: 1.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1 Score: 1.0000\n"
     ]
    }
   ],
   "source": [
    "def calculate_precision(tp, fp):\n",
    "    return tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "\n",
    "def calculate_recall(tp, fn):\n",
    "    return tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "def calculate_f1_score(precision, recall):\n",
    "    return 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "# Call the confusion matrix function\n",
    "tp, tn, fp, fn = calculate_confusion_matrix(genuine_distances, impostor_distances, eer_threshold)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = calculate_accuracy(tp, tn, fp, fn)\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision = calculate_precision(tp, fp)\n",
    "recall = calculate_recall(tp, fn)\n",
    "f1_score = calculate_f1_score(precision, recall)\n",
    "\n",
    "# Print results\n",
    "print(f\"True Positives (TP): {tp}\")\n",
    "print(f\"True Negatives (TN): {tn}\")\n",
    "print(f\"False Positives (FP): {fp}\")\n",
    "print(f\"False Negatives (FN): {fn}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7f9b5fd2-a356-46ad-adc2-5f3d15a330b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encryption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "537931a6-38c3-4dd8-8f25-a36f9442e017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the Keys\n",
    "import utilities\n",
    "import tenseal as ts\n",
    "context = ts.context(\n",
    "    ts.SCHEME_TYPE.CKKS,\n",
    "    poly_modulus_degree=8192,\n",
    "    coeff_mod_bit_sizes = [60, 40, 40,60]\n",
    ")\n",
    "\n",
    "context.generate_galois_keys()\n",
    "context.global_scale=2**40\n",
    "\n",
    "secret_context = context.serialize(save_secret_key = True)\n",
    "utilities.write_data(\"/secret_key.txt\",secret_context) \n",
    "\n",
    "context.make_context_public() # drops private key\n",
    "public_context=context.serialize()\n",
    "utilities.write_data(\"/public_key.txt\",public_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c327e6db-cc83-4ccd-9d02-196ad338d7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def extract_embeddings_from_csv(csv_file, embedding_folder):\n",
    "    # Load the feature vectors from the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "    image_paths = df['Image Name']\n",
    "    feature_vectors = df.iloc[:, 1:].values  # Extract all feature columns\n",
    "\n",
    "    # Create the embedding folder if it doesn't exist\n",
    "    if not os.path.exists(embedding_folder):\n",
    "        os.makedirs(embedding_folder)\n",
    "        \n",
    "    # Count the number of embeddings extracted\n",
    "    total_embeddings = 0\n",
    "\n",
    "    # Iterate over the feature vectors and save the embeddings\n",
    "    for i in range(len(image_paths)):\n",
    "        image_path = image_paths[i]\n",
    "        embedding = feature_vectors[i]\n",
    "\n",
    "        # Extract the folder name from the image path\n",
    "        folder_name = os.path.basename(os.path.dirname(image_path))\n",
    "\n",
    "        # Create the folder structure in the embedding folder\n",
    "        embedding_subfolder = os.path.join(embedding_folder, folder_name)\n",
    "        os.makedirs(embedding_subfolder, exist_ok=True)\n",
    "\n",
    "        # Save the embedding as a numpy file\n",
    "        embedding_file = os.path.join(embedding_subfolder, os.path.splitext(os.path.basename(image_path))[0] + '.npy')\n",
    "        np.save(embedding_file, embedding)\n",
    "        \n",
    "        # Increment the count\n",
    "        total_embeddings += 1\n",
    "\n",
    "    print(\"Embeddings calculated and saved successfully!\")\n",
    "    \n",
    "    return total_embeddings\n",
    "\n",
    "# Set the CSV file\n",
    "csv_file = 'arcface-features.csv'\n",
    "\n",
    " \n",
    "# Set the embedding folder\n",
    "\n",
    "embedding_folder = '/ORL_arcface_Embedding'\n",
    "\n",
    "# Extract embeddings and count the total number of embeddings\n",
    "total_embeddings = extract_embeddings_from_csv(csv_file, embedding_folder)\n",
    "\n",
    "# Print the total number of embeddings\n",
    "print(\"Total number of embeddings:\", total_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d83852c-94d4-4c7e-8c34-dcac01c7272f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import tenseal as ts\n",
    "\n",
    "def encrypt_embedding(embedding_file, embedding_folder, encrypted_folder):\n",
    "    context = ts.context_from(utilities.read_data(\"/secret_key.txt\"))  \n",
    "    embedding = np.load(embedding_file)\n",
    "    embedding = embedding.flatten()  # Flatten the multi-dimensional array to a 1-dimensional vector\n",
    "    enc_embedding = ts.ckks_vector(context, embedding)\n",
    "    enc_embedding_proto = enc_embedding.serialize()\n",
    "    \n",
    "    # Create the folder structure in the encrypted folder\n",
    "    relative_folder = os.path.relpath(os.path.dirname(embedding_file), embedding_folder)\n",
    "    encrypted_embedding_folder = os.path.join(encrypted_folder, relative_folder)\n",
    "    os.makedirs(encrypted_embedding_folder, exist_ok=True)\n",
    "    \n",
    "    # Store the encrypted embedding in the corresponding folder\n",
    "    enc_embedding_file = os.path.join(encrypted_embedding_folder, os.path.basename(embedding_file) + \".txt\")\n",
    "    utilities.write_data(enc_embedding_file, enc_embedding_proto)\n",
    "    \n",
    "    del context, enc_embedding, enc_embedding_proto\n",
    "    return enc_embedding_file\n",
    "\n",
    "# Set the embedding folders \n",
    "\n",
    "embedding_folder = '/ORL_arcface_Embeddings'\n",
    "\n",
    "\n",
    "# Set the encrypted embedding folders\n",
    "encrypted_folder = '/ORL_arcface_Encrypted-Embeddings'\n",
    "\n",
    "\n",
    "# Iterate over the embeddings in the train embedding folder and encrypt them\n",
    "for root, dirs, files in os.walk(embedding_folder):\n",
    "    for file in files:\n",
    "        embedding_file = os.path.join(root, file)\n",
    "        encrypt_embedding(embedding_file, embedding_folder, encrypted_folder)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Encrypted embeddings generated and stored successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a13542-6735-4544-9756-221859c3701d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tenseal as ts\n",
    "import time\n",
    "\n",
    "def encrypt_embedding(embedding_file, embedding_folder, encrypted_folder):\n",
    "    # Start timing for the encryption\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load context and embedding, then perform encryption\n",
    "    context = ts.context_from(utilities.read_data(\"/secret_key.txt\"))  \n",
    "    embedding = np.load(embedding_file)\n",
    "    embedding = embedding.flatten()  # Flatten the multi-dimensional array to a 1-dimensional vector\n",
    "    enc_embedding = ts.ckks_vector(context, embedding)\n",
    "    enc_embedding_proto = enc_embedding.serialize()\n",
    "    \n",
    "    # Create the folder structure in the encrypted folder\n",
    "    relative_folder = os.path.relpath(os.path.dirname(embedding_file), embedding_folder)\n",
    "    encrypted_embedding_folder = os.path.join(encrypted_folder, relative_folder)\n",
    "    os.makedirs(encrypted_embedding_folder, exist_ok=True)\n",
    "    \n",
    "    # Store the encrypted embedding in the corresponding folder\n",
    "    enc_embedding_file = os.path.join(encrypted_embedding_folder, os.path.basename(embedding_file) + \".txt\")\n",
    "    utilities.write_data(enc_embedding_file, enc_embedding_proto)\n",
    "    \n",
    "    # Clean up\n",
    "    del context, enc_embedding, enc_embedding_proto\n",
    "    \n",
    "    # Calculate the elapsed time for encryption\n",
    "    end_time = time.time()\n",
    "    encryption_time = end_time - start_time\n",
    "    \n",
    "    # Return the path to the encrypted embedding and the computation time\n",
    "    return enc_embedding_file, encryption_time\n",
    "\n",
    "# Specify the embedding and encrypted embedding folders\n",
    "\n",
    "embedding_folder = '/ORL_arcface_Embeddings'\n",
    "encrypted_folder = '/ORL_arcface_Encrypted-Embeddings'\n",
    "\n",
    "\n",
    "# Select a single embedding file for testing (change this to the specific file path if needed)\n",
    "test_embedding_file = next(os.path.join(root, file) \n",
    "                           for root, _, files in os.walk(embedding_folder) for file in files)\n",
    "\n",
    "# Encrypt the single embedding and get the computation time\n",
    "enc_file, encryption_time = encrypt_embedding(test_embedding_file, embedding_folder, encrypted_folder)\n",
    "\n",
    "# Print the computation time for this single embedding encryption\n",
    "print(f\"Encrypted {os.path.basename(test_embedding_file)} in {encryption_time:.3f} seconds\")\n",
    "print(f\"Encrypted file saved to: {enc_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c4710967-d57a-4863-a24f-8b76540137d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the Euclidean distance between two encrypted embeddings\n",
    "def calculate_distance(enc_embedding_file1, enc_embedding_file2):\n",
    "    context = ts.context_from(utilities.read_data(\"/public_key.txt\"))\n",
    "    enc_embedding_proto1 = utilities.read_data(enc_embedding_file1)\n",
    "    enc_embedding_proto2 = utilities.read_data(enc_embedding_file2)\n",
    "    enc_embedding1 = ts.lazy_ckks_vector_from(enc_embedding_proto1)\n",
    "    enc_embedding1.link_context(context)\n",
    "    enc_embedding2 = ts.lazy_ckks_vector_from(enc_embedding_proto2)\n",
    "    enc_embedding2.link_context(context)\n",
    "    euclidean_squared = enc_embedding1 - enc_embedding2\n",
    "    euclidean_squared = euclidean_squared.dot(euclidean_squared)\n",
    "    euclidean_squared_file = os.path.join(euclidean_folder, os.path.basename(enc_embedding_file1) + \"_-_\" +\n",
    "                                          os.path.basename(enc_embedding_file2) + \".txt\")\n",
    "    utilities.write_data(euclidean_squared_file, euclidean_squared.serialize())\n",
    "    return euclidean_squared_file\n",
    "\n",
    "# Function to decrypt and compare the Euclidean distance\n",
    "def decrypt_and_compare(euclidean_squared_file):\n",
    "    if euclidean_squared_file is None:\n",
    "        return None  # Skip processing if the file path is None\n",
    "\n",
    "    context = ts.context_from(utilities.read_data(\"/secret_key.txt\"))\n",
    "    euclidean_squared_proto = utilities.read_data(euclidean_squared_file)\n",
    "    euclidean_squared = ts.lazy_ckks_vector_from(euclidean_squared_proto)\n",
    "    euclidean_squared.link_context(context)\n",
    "    euclidean_squared_plain = euclidean_squared.decrypt()[0]\n",
    "    return euclidean_squared_plain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "74a4ce03-690d-43e4-bc68-51dd4dd94056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genuine-Distance Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efe0f37-078d-4179-998c-1961eef13b4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import tenseal as ts\n",
    "import utilities\n",
    "\n",
    "def calculate_distance(enc_embedding_file1, enc_embedding_file2):\n",
    "    context = ts.context_from(utilities.read_data(\"/public_key.txt\"))\n",
    "    enc_embedding_proto1 = utilities.read_data(enc_embedding_file1)\n",
    "    enc_embedding_proto2 = utilities.read_data(enc_embedding_file2)\n",
    "    enc_embedding1 = ts.lazy_ckks_vector_from(enc_embedding_proto1)\n",
    "    enc_embedding1.link_context(context)\n",
    "    enc_embedding2 = ts.lazy_ckks_vector_from(enc_embedding_proto2)\n",
    "    enc_embedding2.link_context(context)\n",
    "\n",
    "    # Calculate the squared Euclidean distance between the two encrypted vectors\n",
    "    euclidean_squared = enc_embedding1 - enc_embedding2\n",
    "    euclidean_squared = euclidean_squared.dot(euclidean_squared)\n",
    "\n",
    "    # Save the encrypted Euclidean distance result in a file\n",
    "    euclidean_squared_file = os.path.join(euclidean_folder, f\"{os.path.basename(enc_embedding_file1)}_-_{os.path.basename(enc_embedding_file2)}.txt\")\n",
    "    utilities.write_data(euclidean_squared_file, euclidean_squared.serialize())\n",
    "    \n",
    "    return euclidean_squared_file\n",
    "\n",
    "\n",
    "# Set the folder paths\n",
    "\n",
    "embedding_folder = '/ORL_arcface_Embeddings'\n",
    "euclidean_folder = '/ORL_arcface_Genuine-Euclidean-Distances'\n",
    "\n",
    "\n",
    "# Create the Euclidean distance folder if it doesn't exist\n",
    "os.makedirs(euclidean_folder, exist_ok=True)\n",
    "\n",
    "# Get the list of class folders inside the encrypted embeddings folder\n",
    "class_folders = glob.glob(os.path.join(encrypted_folder, '*'))\n",
    "\n",
    "# Iterate over each class folder\n",
    "for class_folder in class_folders:\n",
    "    # Get the list of embedding files in the current class folder\n",
    "    embedding_files = glob.glob(os.path.join(class_folder, '*.txt'))\n",
    "\n",
    "    # Ensure that there are exactly 2 embeddings in each class folder for genuine distance calculation\n",
    "    if len(embedding_files) == 2:\n",
    "        # Calculate the Euclidean distance between the two embeddings\n",
    "        euclidean_squared_file = calculate_distance(embedding_files[0], embedding_files[1])\n",
    "\n",
    "        # Extract image names for logging\n",
    "        image1_name = os.path.splitext(os.path.basename(embedding_files[0]))[0]\n",
    "        image2_name = os.path.splitext(os.path.basename(embedding_files[1]))[0]\n",
    "\n",
    "        # Print the results to the console\n",
    "        print(f\"Class: {os.path.basename(class_folder)}, Image 1: {image1_name}, Image 2: {image2_name}, Euclidean Distance File: {euclidean_squared_file}\")\n",
    "\n",
    "print(\"Genuine distance calculation complete. Encrypted distances are stored in the specified folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "7fe96396-13a6-45ce-ba6a-3c1e840f8e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decrypt the genuine distances and store them in a csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32991760-0529-4a0a-8ee2-02bbe09f5fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import tenseal as ts\n",
    "import utilities\n",
    "import csv\n",
    "\n",
    "def decrypt_and_generate_distances(euclidean_folder):\n",
    "    decrypted_distances = []\n",
    "    \n",
    "    # Loop through each encrypted file in the specified folder\n",
    "    for encrypted_file in glob.glob(os.path.join(euclidean_folder, '*.txt')):\n",
    "        try:\n",
    "            # Load the encryption context using the secret key\n",
    "            context = ts.context_from(utilities.read_data(\"/secret_key.txt\"))\n",
    "            \n",
    "            # Load the encrypted distance data\n",
    "            encrypted_distance_proto = utilities.read_data(encrypted_file)\n",
    "            encrypted_distance = ts.lazy_ckks_vector_from(encrypted_distance_proto)\n",
    "            encrypted_distance.link_context(context)\n",
    "            \n",
    "            # Decrypt the distance and extract the first value (scalar)\n",
    "            decrypted_distance = encrypted_distance.decrypt()[0]\n",
    "            \n",
    "            # Append the decrypted distance to the list\n",
    "            decrypted_distances.append(decrypted_distance)\n",
    "        except Exception as e:\n",
    "            # Handle decryption errors gracefully\n",
    "            print(f\"Error decrypting {encrypted_file}: {str(e)}\")\n",
    "\n",
    "    # Define the path for the output CSV file\n",
    "    csv_file = os.path.join(euclidean_folder, \"/ORL_arcface_Genuine-Euclidean-Distances.csv\")\n",
    "    \n",
    "    # Write the decrypted distances to a CSV file\n",
    "    with open(csv_file, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Distance\"])  # CSV header\n",
    "        writer.writerows([[distance] for distance in decrypted_distances])  # Write each distance\n",
    "\n",
    "    print(f\"Decryption and distance generation completed. Total distances: {len(decrypted_distances)}\")\n",
    "\n",
    "# Set the folder containing the encrypted distance files\n",
    "# euclidean_folder = 'D:/Securing MULTIMODAL biometrics using FHE/PAPER2 LATEST/FHE/arcface_Genuine-Euclidean-Distances_LPP16_FHE'\n",
    "\n",
    "euclidean_folder = '/ORL_arcface_Genuine-Euclidean-Distances'\n",
    "\n",
    "# Decrypt and generate distances\n",
    "decrypt_and_generate_distances(euclidean_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5996555a-982e-4c6a-9191-b3124f3d65ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d199dbde-e5e0-4e27-b4fb-0b2342477559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decryption and distance generation completed. Total distances: 41\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import tenseal as ts\n",
    "import utilities\n",
    "import csv\n",
    "\n",
    "def decrypt_and_generate_distances(euclidean_folder):\n",
    "    decrypted_distances = []\n",
    "    \n",
    "    # Loop through each encrypted file in the specified folder\n",
    "    for encrypted_file in glob.glob(os.path.join(euclidean_folder, '*.txt')):\n",
    "        try:\n",
    "            # Load the encryption context using the secret key\n",
    "            context = ts.context_from(utilities.read_data(\"/secret_key.txt\"))\n",
    "            \n",
    "            # Load the encrypted distance data\n",
    "            encrypted_distance_proto = utilities.read_data(encrypted_file)\n",
    "            encrypted_distance = ts.lazy_ckks_vector_from(encrypted_distance_proto)\n",
    "            encrypted_distance.link_context(context)\n",
    "            \n",
    "            # Decrypt the distance and extract the first value (scalar)\n",
    "            decrypted_distance = encrypted_distance.decrypt()[0]\n",
    "            \n",
    "            # Append the decrypted distance to the list\n",
    "            decrypted_distances.append(decrypted_distance)\n",
    "        except Exception as e:\n",
    "            # Handle decryption errors gracefully\n",
    "            print(f\"Error decrypting {encrypted_file}: {str(e)}\")\n",
    "\n",
    "    # Define the path for the output CSV file\n",
    "    \n",
    "    #csv_file = 'D:/Securing MULTIMODAL biometrics using FHE/FHE/arcFace-Genuine-Distances_without_Lpp.csv'\n",
    "    csv_file = '/ORL_arcface_Genuine-Euclidean-Distances.csv'\n",
    "    \n",
    "    # Write the decrypted distances to a CSV file\n",
    "    with open(csv_file, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Distance\"])  # CSV header\n",
    "        writer.writerows([[distance] for distance in decrypted_distances])  # Write each distance\n",
    "\n",
    "    print(f\"Decryption and distance generation completed. Total distances: {len(decrypted_distances)}\")\n",
    "\n",
    "# Set the folder containing the encrypted distance files\n",
    "euclidean_folder = '/ORL_arcface_Genuine-Euclidean-Distances'\n",
    "\n",
    "# Decrypt and generate distances\n",
    "decrypt_and_generate_distances(euclidean_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1d1d9cd9-3f2b-42b6-9372-bf6e285fc6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMpostor Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c86522ec-49d4-4068-b03a-f25ead4c2de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decryption and distance generation completed. Total distances: 3280\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import tenseal as ts\n",
    "import utilities\n",
    "import csv\n",
    "\n",
    "def decrypt_and_generate_distances(euclidean_folder):\n",
    "    decrypted_distances = []\n",
    "    \n",
    "    # Loop through each encrypted file in the specified folder\n",
    "    for encrypted_file in glob.glob(os.path.join(euclidean_folder, '*.txt')):\n",
    "        try:\n",
    "            # Load the encryption context using the secret key\n",
    "            context = ts.context_from(utilities.read_data(\"/secret_key.txt\"))\n",
    "            \n",
    "            # Load the encrypted distance data\n",
    "            encrypted_distance_proto = utilities.read_data(encrypted_file)\n",
    "            encrypted_distance = ts.lazy_ckks_vector_from(encrypted_distance_proto)\n",
    "            encrypted_distance.link_context(context)\n",
    "            \n",
    "            # Decrypt the distance and extract the first value (scalar)\n",
    "            decrypted_distance = encrypted_distance.decrypt()[0]\n",
    "            \n",
    "            # Append the decrypted distance to the list\n",
    "            decrypted_distances.append(decrypted_distance)\n",
    "        except Exception as e:\n",
    "            # Handle decryption errors gracefully\n",
    "            print(f\"Error decrypting {encrypted_file}: {str(e)}\")\n",
    "\n",
    "    # Define the path for the output CSV file\n",
    "    \n",
    "    csv_file = '/ORL_arcface_Impostor-Euclidean-Distances.csv'\n",
    "    \n",
    "    # Write the decrypted distances to a CSV file\n",
    "    with open(csv_file, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Distance\"])  # CSV header\n",
    "        writer.writerows([[distance] for distance in decrypted_distances])  # Write each distance\n",
    "\n",
    "    print(f\"Decryption and distance generation completed. Total distances: {len(decrypted_distances)}\")\n",
    "\n",
    "# Set the folder containing the encrypted distance files\n",
    "\n",
    "euclidean_folder = '/ORL_arcface_Impostor-Euclidean-Distances'\n",
    "\n",
    "# Decrypt and generate distances\n",
    "decrypt_and_generate_distances(euclidean_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c9010e-c67d-49bf-a87a-1553655eb188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load distances from CSV files\n",
    "def load_distances(csv_filename):\n",
    "    distances = []\n",
    "    with open(csv_filename, 'r') as csvfile:\n",
    "        csv_reader = csv.reader(csvfile)\n",
    "        next(csv_reader)  # Skip header\n",
    "        for row in csv_reader:\n",
    "            dist = float(row[0])\n",
    "            distances.append(dist)\n",
    "    return distances\n",
    "\n",
    "# Calculate FAR, FRR, and EER\n",
    "def calculate_far_frr_eer(genuine_distances, impostor_distances):\n",
    "    genuine_distances = np.array(genuine_distances)\n",
    "    impostor_distances = np.array(impostor_distances)\n",
    "    \n",
    "    min_threshold = min(np.min(genuine_distances), np.min(impostor_distances))\n",
    "    max_threshold = max(np.max(genuine_distances), np.max(impostor_distances))\n",
    "    \n",
    "    thresholds = np.linspace(min_threshold, max_threshold, num=1000)\n",
    "    \n",
    "    frr = np.zeros(len(thresholds))\n",
    "    far = np.zeros(len(thresholds))\n",
    "\n",
    "    for i, threshold in enumerate(thresholds):\n",
    "        frr[i] = np.sum(genuine_distances > threshold) / len(genuine_distances)\n",
    "        far[i] = np.sum(impostor_distances <= threshold) / len(impostor_distances)\n",
    "        \n",
    "    eer_index = np.abs(frr - far).argmin()\n",
    "    eer = np.mean((frr[eer_index], far[eer_index]))\n",
    "    \n",
    "    return far, frr, eer, thresholds[eer_index], thresholds\n",
    "\n",
    "# File paths\n",
    "\n",
    "genuine_output_filename = '/ORL_arcface_Genuine-Euclidean-Distances.csv'\n",
    "impostor_output_filename = '/ORL_arcface_Impostor-Euclidean-Distances.csv'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load distances from CSV files\n",
    "genuine_distances = load_distances(genuine_output_filename)\n",
    "impostor_distances = load_distances(impostor_output_filename)\n",
    "\n",
    "# Calculate FAR, FRR, and EER\n",
    "far, frr, eer, eer_threshold, thresholds = calculate_far_frr_eer(genuine_distances, impostor_distances)\n",
    "\n",
    "# Print results\n",
    "print(f\"EER: {eer}\")\n",
    "print(f\"Threshold at EER: {eer_threshold}\")\n",
    "\n",
    "# Plot FAR and FRR\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, far, label='FAR', color='red')\n",
    "plt.plot(thresholds, frr, label='FRR', color='blue')\n",
    "plt.axvline(x=eer_threshold, linestyle='--', color='green', label=f'EER Threshold: {eer_threshold:.4f}')\n",
    "plt.axhline(y=eer, linestyle='--', color='purple', label=f'EER: {eer:.4f}')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Rate')\n",
    "plt.title('FAR and FRR vs. Threshold')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Save the plot\n",
    "output_plot_filename = '/ORL_arcface_euclidean_EER.png'\n",
    "\n",
    "plt.savefig(output_plot_filename)\n",
    "\n",
    "print(f\"Plot saved as {output_plot_filename}\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86455bab-f9ab-4f40-bcc6-3595f314f1eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
