{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47451e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Facenet Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e44164e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90263c90-d231-4164-85bb-45ea24fbd13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tenseal==0.3.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f9175e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install torch torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e144c6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch torchvision facenet-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6d4c19-3b4e-47c8-9bb8-97b8e18e67cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install torch==2.2.2 torchvision==0.17.2 facenet-pytorch==2.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6022480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from facenet_pytorch import InceptionResnetV1\n",
    "from torchvision import transforms\n",
    "\n",
    "# Load the pre-trained FaceNet model\n",
    "model = InceptionResnetV1(pretrained='vggface2').eval()\n",
    "\n",
    "# Define the preprocessing transformation\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((160, 160)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Function to extract features from an image\n",
    "def extract_features(image_path, model):\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img = preprocess(img).unsqueeze(0)  # Add batch dimension\n",
    "    with torch.no_grad():\n",
    "        features = model(img).squeeze().numpy()  # Forward pass to get features\n",
    "    return features\n",
    "\n",
    "# Directory containing the face images dataset\n",
    "input_directory = '/dataset/path'  \n",
    "\n",
    "# Output file where features will be saved\n",
    "output_filename = 'facenet_features.csv'\n",
    "\n",
    "\n",
    "# Prepare CSV file for writing\n",
    "with open(output_filename, 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    \n",
    "    # Write headers\n",
    "    headers = ['Image Name'] + [f'f{i+1}' for i in range(512)]  # FaceNet outputs 512-dimensional features\n",
    "    csv_writer.writerow(headers)\n",
    "    \n",
    "    # Process each image in the dataset\n",
    "    for class_dir in os.listdir(input_directory):\n",
    "        class_path = os.path.join(input_directory, class_dir)\n",
    "        if os.path.isdir(class_path):\n",
    "            for file in os.listdir(class_path):\n",
    "                if file.endswith('.jpg') or file.endswith('.png') or file.endswith('.bmp'):\n",
    "                    file_path = os.path.join(class_path, file)\n",
    "                    \n",
    "                    # Extract features\n",
    "                    features = extract_features(file_path, model)\n",
    "                    \n",
    "                    # Write the features to the CSV file\n",
    "                    image_name = f'{class_dir}/{file}'\n",
    "                    csv_writer.writerow([image_name] + features.tolist())\n",
    "\n",
    "print(\"Feature extraction and saving complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143f34a6-b0c8-49b2-a7ee-28fd7b20a8ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02612989-2692-47bd-bc5a-3218ad61206e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.linalg import eigh\n",
    "\n",
    "# Load the face features from the CSV file\n",
    "csv_path = 'facenet_features.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Extract the feature vectors (ignoring the first column with image names)\n",
    "X = df.iloc[:, 1:].values\n",
    "\n",
    "# LPP function\n",
    "def compute_weight_matrix(X, epsilon):\n",
    "    distances = squareform(pdist(X.T, 'euclidean'))\n",
    "    W = np.zeros_like(distances)\n",
    "    W[distances <= epsilon] = 1\n",
    "    return W\n",
    "\n",
    "def compute_laplacian_matrix(W):\n",
    "    D = np.diag(np.sum(W, axis=1))\n",
    "    L = D - W\n",
    "    return L\n",
    "\n",
    "def lpp(X, num_components, epsilon):\n",
    "    W = compute_weight_matrix(X, epsilon)\n",
    "    L = compute_laplacian_matrix(W)\n",
    "    D = np.diag(np.sum(W, axis=1))\n",
    "    \n",
    "    # Eigen decomposition of the generalized eigenvalue problem\n",
    "    eigvals, eigvecs = eigh(L, D)\n",
    "    \n",
    "    # Sort the eigenvectors by eigenvalues\n",
    "    sorted_indices = np.argsort(eigvals)\n",
    "    eigvecs = eigvecs[:, sorted_indices]\n",
    "    \n",
    "    # Select the top `num_components` eigenvectors\n",
    "    V = eigvecs[:, :num_components]\n",
    "    \n",
    "    # Project the original data using the selected eigenvectors\n",
    "    X_reduced = np.dot(X, V)  \n",
    "    return X_reduced\n",
    "\n",
    "# Parameters for LPP\n",
    "num_components = 256  # Reduce to 256 dimensions\n",
    "epsilon = 0.5         # You may need to tune this threshold\n",
    "\n",
    "# Apply LPP\n",
    "X_reduced = lpp(X, num_components, epsilon)\n",
    "\n",
    "# Create a DataFrame to save the reduced features\n",
    "reduced_df = pd.DataFrame(X_reduced, columns=[f'f{i+1}' for i in range(num_components)])\n",
    "\n",
    "# Insert the image names back into the DataFrame\n",
    "reduced_df.insert(0, 'Image Name', df['Image Name'])\n",
    "\n",
    "# Save the reduced features to a new CSV file\n",
    "output_path = 'facenet_features_Lpp_dim.csv' # dim = (8,16,32,64,128,256)\n",
    "reduced_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"LPP feature reduction completed and saved to:\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b05b40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "de9b54d7",
   "metadata": {},
   "source": [
    "Key Improvements and Explanations:\n",
    "\n",
    "Flexibility in Graph Construction: The code now supports both epsilon-neighborhood and k-Nearest Neighbors (k_neighbors) for constructing the adjacency graph. You must provide either epsilon or k_neighbors, but not both. The k-NN approach is implemented using np.argsort to find the nearest neighbors efficiently.\n",
    "\n",
    "Corrected Eigen Decomposition: The code now correctly calculates XLXT and XDXT and uses them in the eigh function. It also handles the case where the number of features is larger than the number of samples by using eig instead of eigh to avoid memory issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091f17ed-63cb-4408-9556-d139255f6ae3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b27b84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find genuine and impostors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3589203",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import euclidean\n",
    "from itertools import combinations\n",
    "\n",
    "# Load the extracted features\n",
    "def load_features(csv_filename):\n",
    "    features = {}\n",
    "    with open(csv_filename, 'r') as csvfile:\n",
    "        csv_reader = csv.reader(csvfile)\n",
    "        headers = next(csv_reader)  # Skip header\n",
    "        for row in csv_reader:\n",
    "            image_name = row[0]\n",
    "            feature_vector = np.array(row[1:], dtype=float)\n",
    "            features[image_name] = feature_vector\n",
    "    return features\n",
    "\n",
    "# Calculate Euclidean distance and classify pairs\n",
    "def classify_pairs(features):\n",
    "    genuine_pairs = []\n",
    "    impostor_pairs = []\n",
    "    \n",
    "    all_pairs = list(combinations(features.keys(), 2))\n",
    "    \n",
    "    for (image1, image2) in all_pairs:\n",
    "        class1 = image1.split('/')[0]\n",
    "        class2 = image2.split('/')[0]\n",
    "        dist = euclidean(features[image1], features[image2])\n",
    "        \n",
    "        if class1 == class2:\n",
    "            genuine_pairs.append((image1, image2, dist))\n",
    "        else:\n",
    "            impostor_pairs.append((image1, image2, dist))\n",
    "    \n",
    "    return genuine_pairs, impostor_pairs\n",
    "\n",
    "# Save pairs to CSV files\n",
    "def save_pairs_to_csv(pairs, filename):\n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow(['Image Pair', 'Distance'])\n",
    "        for (image1, image2, dist) in pairs:\n",
    "            csv_writer.writerow([f\"{image1} - {image2}\", dist])\n",
    "\n",
    "\n",
    "features_filename = \"facenet_features_Lpp_dim.csv\"\n",
    "\n",
    "\n",
    "genuine_output_filename = \"facenet_Genuine.csv\"\n",
    "impostor_output_filename = \"facenet_Impostor.csv\"\n",
    "\n",
    "\n",
    "\n",
    "# Load features\n",
    "features = load_features(features_filename)\n",
    "\n",
    "# Classify pairs\n",
    "genuine_pairs, impostor_pairs = classify_pairs(features)\n",
    "\n",
    "# Save pairs to CSV files\n",
    "save_pairs_to_csv(genuine_pairs, genuine_output_filename)\n",
    "save_pairs_to_csv(impostor_pairs, impostor_output_filename)\n",
    "\n",
    "# Print the count of genuine and impostor matches\n",
    "print(f\"Number of genuine matches: {len(genuine_pairs)}\")\n",
    "print(f\"Number of impostor matches: {len(impostor_pairs)}\")\n",
    "\n",
    "print(\"Genuine and impostor matches calculation and saving complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f64cd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find and plot FAR , FRR and EER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37697050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load distances from CSV files\n",
    "def load_distances(csv_filename):\n",
    "    distances = []\n",
    "    with open(csv_filename, 'r') as csvfile:\n",
    "        csv_reader = csv.reader(csvfile)\n",
    "        next(csv_reader)  # Skip header\n",
    "        for row in csv_reader:\n",
    "            dist = float(row[1])\n",
    "            distances.append(dist)\n",
    "    return distances\n",
    "\n",
    "# Calculate FAR, FRR, and EER\n",
    "def calculate_far_frr_eer(genuine_distances, impostor_distances):\n",
    "    genuine_distances = np.array(genuine_distances)\n",
    "    impostor_distances = np.array(impostor_distances)\n",
    "    \n",
    "    min_threshold = min(np.min(genuine_distances), np.min(impostor_distances))\n",
    "    max_threshold = max(np.max(genuine_distances), np.max(impostor_distances))\n",
    "    \n",
    "    thresholds = np.linspace(min_threshold, max_threshold, num=1000)\n",
    "    \n",
    "    frr = np.zeros(len(thresholds))\n",
    "    far = np.zeros(len(thresholds))\n",
    "\n",
    "    for i, threshold in enumerate(thresholds):\n",
    "        frr[i] = np.sum(genuine_distances > threshold) / len(genuine_distances)\n",
    "        far[i] = np.sum(impostor_distances <= threshold) / len(impostor_distances)\n",
    "        \n",
    "    eer_index = np.abs(frr - far).argmin()\n",
    "    eer = np.mean((frr[eer_index], far[eer_index]))\n",
    "    \n",
    "    return far, frr, eer, thresholds[eer_index], thresholds\n",
    "\n",
    "# File paths\n",
    "# genuine_output_filename = 'D:/Securing MULTIMODAL biometrics using FHE/PAPER2 LATEST/Scores/Genuine_facenet_512.csv'\n",
    "# impostor_output_filename = 'D:/Securing MULTIMODAL biometrics using FHE/PAPER2 LATEST/Scores/Impostor_facenet_512.csv'\n",
    "\n",
    "\n",
    "\n",
    "genuine_output_filename = \"facenet_Genuine_256.csv\"\n",
    "impostor_output_filename = \"facenet_Impostor_256.csv\"\n",
    "\n",
    "# Load distances from CSV files\n",
    "genuine_distances = load_distances(genuine_output_filename)\n",
    "impostor_distances = load_distances(impostor_output_filename)\n",
    "\n",
    "# Calculate FAR, FRR, and EER\n",
    "far, frr, eer, eer_threshold, thresholds = calculate_far_frr_eer(genuine_distances, impostor_distances)\n",
    "\n",
    "# Print results\n",
    "print(f\"EER: {eer}\")\n",
    "print(f\"Threshold at EER: {eer_threshold}\")\n",
    "\n",
    "# Plot FAR and FRR\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, far, label='FAR', color='red')\n",
    "plt.plot(thresholds, frr, label='FRR', color='blue')\n",
    "plt.axvline(x=eer_threshold, linestyle='--', color='green', label=f'EER Threshold: {eer_threshold:.4f}')\n",
    "plt.axhline(y=eer, linestyle='--', color='purple', label=f'EER: {eer:.4f}')\n",
    "plt.xlabel('Threshold',  fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Rate',  fontsize=14, fontweight='bold')\n",
    "plt.title('FAR and FRR vs. Threshold',  fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Save the plot\n",
    "output_plot_filename = 'plot.png'\n",
    "plt.savefig(output_plot_filename)\n",
    "\n",
    "print(f\"Plot saved as {output_plot_filename}\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d518c0f5-fde9-4379-944b-f91a180b47f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_confusion_matrix(genuine_distances, impostor_distances, threshold):\n",
    "    # Convert lists to NumPy arrays\n",
    "    genuine_distances = np.array(genuine_distances)\n",
    "    impostor_distances = np.array(impostor_distances)\n",
    "    \n",
    "    tp = np.sum(genuine_distances <= threshold)  # True Positives\n",
    "    fn = np.sum(genuine_distances > threshold)   # False Negatives\n",
    "    tn = np.sum(impostor_distances > threshold)  # True Negatives\n",
    "    fp = np.sum(impostor_distances <= threshold) # False Positives\n",
    "    \n",
    "    return tp, tn, fp, fn\n",
    "\n",
    "def calculate_accuracy(tp, tn, fp, fn):\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    return accuracy\n",
    "\n",
    "# Assuming you've already calculated the threshold at EER using the existing code\n",
    "eer_threshold = 0.7\n",
    " \n",
    "\n",
    "# Call the confusion matrix function\n",
    "tp, tn, fp, fn = calculate_confusion_matrix(genuine_distances, impostor_distances, eer_threshold)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = calculate_accuracy(tp, tn, fp, fn)\n",
    "\n",
    "# Print results\n",
    "print(f\"True Positives (TP): {tp}\")\n",
    "print(f\"True Negatives (TN): {tn}\")\n",
    "print(f\"False Positives (FP): {fp}\")\n",
    "print(f\"False Negatives (FN): {fn}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71519f6d-491f-4451-a6ba-155abedc193d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision(tp, fp):\n",
    "    return tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "\n",
    "def calculate_recall(tp, fn):\n",
    "    return tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "def calculate_f1_score(precision, recall):\n",
    "    return 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "# Call the confusion matrix function\n",
    "tp, tn, fp, fn = calculate_confusion_matrix(genuine_distances, impostor_distances, eer_threshold)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = calculate_accuracy(tp, tn, fp, fn)\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision = calculate_precision(tp, fp)\n",
    "recall = calculate_recall(tp, fn)\n",
    "f1_score = calculate_f1_score(precision, recall)\n",
    "\n",
    "# Print results\n",
    "print(f\"True Positives (TP): {tp}\")\n",
    "print(f\"True Negatives (TN): {tn}\")\n",
    "print(f\"False Positives (FP): {fp}\")\n",
    "print(f\"False Negatives (FN): {fn}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9b3e4f-47d9-46ae-9c58-fe8cffecd5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curves "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0db4f4-d6c7-4181-b42d-adeaa1fc266a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Load distances from CSV files\n",
    "def load_distances(csv_filename):\n",
    "    distances = []\n",
    "    with open(csv_filename, 'r') as csvfile:\n",
    "        csv_reader = csv.reader(csvfile)\n",
    "        next(csv_reader)  # Skip header\n",
    "        for row in csv_reader:\n",
    "            dist = float(row[1])\n",
    "            distances.append(dist)\n",
    "    return distances\n",
    "\n",
    "# File paths for various sizes\n",
    "sizes = [8, 16, 32, 64, 128, 246, 256]\n",
    "genuine_files = [f'Genuine_facenet_{size}.csv' for size in sizes]\n",
    "impostor_files = [f'Impostor_facenet_{size}.csv' for size in sizes]\n",
    "\n",
    "# Plot ROC curves\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, size in enumerate(sizes):\n",
    "    genuine_distances = load_distances(genuine_files[i])\n",
    "    impostor_distances = load_distances(impostor_files[i])\n",
    "    \n",
    "    # Create labels: genuine as 1, impostor as 0\n",
    "    y_true = np.concatenate([np.ones(len(genuine_distances)), np.zeros(len(impostor_distances))])\n",
    "    scores = np.concatenate([genuine_distances, impostor_distances])\n",
    "\n",
    "    # Calculate FAR (FPR) and TPR (1 - FRR) for ROC\n",
    "    fpr, tpr, _ = roc_curve(y_true, scores, pos_label=0)\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    plt.plot(fpr, tpr, label=f'Size {size}', linewidth=2)\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('False Acceptance Rate (FAR)%', fontsize=14, fontweight='bold') \n",
    "plt.ylabel('True Positive Rate (1 - FRR)%', fontsize=14, fontweight='bold')\n",
    "plt.title('ROC Curves for Different Feature Sizes')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, linestyle='--', color='gray', alpha=0.5)\n",
    "\n",
    "# Save the plot\n",
    "output_plot_filename = 'roc_curves_facenet_Lpp.png'\n",
    "plt.savefig(output_plot_filename, dpi=300, format='png', bbox_inches='tight')\n",
    "\n",
    "print(f\"ROC curves plot saved as {output_plot_filename}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb89a32-a6ab-4c22-a4fa-392132a1e360",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330e4840-058d-44af-9211-96466408da5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a988b2-4cb0-4961-b29b-da0aa7a4bafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encryption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bd1979-f649-40a6-bfac-1780bd1bc724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the Keys\n",
    "import utilities\n",
    "import tenseal as ts\n",
    "context = ts.context(\n",
    "    ts.SCHEME_TYPE.CKKS,\n",
    "    poly_modulus_degree=8192,\n",
    "    coeff_mod_bit_sizes = [60, 40, 40,60]\n",
    ")\n",
    "\n",
    "context.generate_galois_keys()\n",
    "context.global_scale=2**40\n",
    "\n",
    "secret_context = context.serialize(save_secret_key = True)\n",
    "utilities.write_data(\"/secret_key.txt\",secret_context) \n",
    "\n",
    "context.make_context_public() # drops private key\n",
    "public_context=context.serialize()\n",
    "utilities.write_data(\"/public_key.txt\",public_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6faf4dc-a9b2-42c9-b22c-3a9e6758433e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def extract_embeddings_from_csv(csv_file, embedding_folder):\n",
    "    # Load the feature vectors from the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "    image_paths = df['Image Name']\n",
    "    feature_vectors = df.iloc[:, 1:].values  # Extract all feature columns\n",
    "\n",
    "    # Create the embedding folder if it doesn't exist\n",
    "    if not os.path.exists(embedding_folder):\n",
    "        os.makedirs(embedding_folder)\n",
    "        \n",
    "    # Count the number of embeddings extracted\n",
    "    total_embeddings = 0\n",
    "\n",
    "    # Iterate over the feature vectors and save the embeddings\n",
    "    for i in range(len(image_paths)):\n",
    "        image_path = image_paths[i]\n",
    "        embedding = feature_vectors[i]\n",
    "\n",
    "        # Extract the folder name from the image path\n",
    "        folder_name = os.path.basename(os.path.dirname(image_path))\n",
    "\n",
    "        # Create the folder structure in the embedding folder\n",
    "        embedding_subfolder = os.path.join(embedding_folder, folder_name)\n",
    "        os.makedirs(embedding_subfolder, exist_ok=True)\n",
    "\n",
    "        # Save the embedding as a numpy file\n",
    "        embedding_file = os.path.join(embedding_subfolder, os.path.splitext(os.path.basename(image_path))[0] + '.npy')\n",
    "        np.save(embedding_file, embedding)\n",
    "        \n",
    "        # Increment the count\n",
    "        total_embeddings += 1\n",
    "\n",
    "    print(\"Embeddings calculated and saved successfully!\")\n",
    "    \n",
    "    return total_embeddings\n",
    "\n",
    "# Set the CSV file\n",
    "\n",
    "csv_file = '/facenet_features_lpp_246.csv'\n",
    "\n",
    "# Set the embedding folder\n",
    "\n",
    "embedding_folder = '/casia_Embeddings_246'\n",
    "\n",
    "# Extract embeddings and count the total number of embeddings\n",
    "total_embeddings = extract_embeddings_from_csv(csv_file, embedding_folder)\n",
    "\n",
    "# Print the total number of embeddings\n",
    "print(\"Total number of embeddings:\", total_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bf3f2c-c044-43a7-8ef0-b8c64766aaec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c790aa-d0d0-4a8c-891e-d12e69d70af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import tenseal as ts\n",
    "\n",
    "def encrypt_embedding(embedding_file, embedding_folder, encrypted_folder):\n",
    "    context = ts.context_from(utilities.read_data(\"/secret_key.txt\"))  \n",
    "    embedding = np.load(embedding_file)\n",
    "    embedding = embedding.flatten()  # Flatten the multi-dimensional array to a 1-dimensional vector\n",
    "    enc_embedding = ts.ckks_vector(context, embedding)\n",
    "    enc_embedding_proto = enc_embedding.serialize()\n",
    "    \n",
    "    # Create the folder structure in the encrypted folder\n",
    "    relative_folder = os.path.relpath(os.path.dirname(embedding_file), embedding_folder)\n",
    "    encrypted_embedding_folder = os.path.join(encrypted_folder, relative_folder)\n",
    "    os.makedirs(encrypted_embedding_folder, exist_ok=True)\n",
    "    \n",
    "    # Store the encrypted embedding in the corresponding folder\n",
    "    enc_embedding_file = os.path.join(encrypted_embedding_folder, os.path.basename(embedding_file) + \".txt\")\n",
    "    utilities.write_data(enc_embedding_file, enc_embedding_proto)\n",
    "    \n",
    "    del context, enc_embedding, enc_embedding_proto\n",
    "    return enc_embedding_file\n",
    "\n",
    "# Set the embedding folders \n",
    "\n",
    "embedding_folder = '/casia_Embeddings_246'\n",
    "\n",
    "\n",
    "# Set the encrypted embedding folders\n",
    "encrypted_folder = '/casia_Encrypted-Embeddings_246/'\n",
    "\n",
    "\n",
    "# Iterate over the embeddings in the train embedding folder and encrypt them\n",
    "for root, dirs, files in os.walk(embedding_folder):\n",
    "    for file in files:\n",
    "        embedding_file = os.path.join(root, file)\n",
    "        encrypt_embedding(embedding_file, embedding_folder, encrypted_folder)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Encrypted embeddings generated and stored successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74249124-24cd-4237-83bc-4a3fd618ce7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e38080-ce95-4919-bbc0-afc5b54dc97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tenseal as ts\n",
    "import time\n",
    "\n",
    "def encrypt_embedding(embedding_file, embedding_folder, encrypted_folder):\n",
    "    # Start timing for the encryption\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load context and embedding, then perform encryption\n",
    "    context = ts.context_from(utilities.read_data(\"/secret_key.txt\"))  \n",
    "    embedding = np.load(embedding_file)\n",
    "    embedding = embedding.flatten()  # Flatten the multi-dimensional array to a 1-dimensional vector\n",
    "    enc_embedding = ts.ckks_vector(context, embedding)\n",
    "    enc_embedding_proto = enc_embedding.serialize()\n",
    "    \n",
    "    # Create the folder structure in the encrypted folder\n",
    "    relative_folder = os.path.relpath(os.path.dirname(embedding_file), embedding_folder)\n",
    "    encrypted_embedding_folder = os.path.join(encrypted_folder, relative_folder)\n",
    "    os.makedirs(encrypted_embedding_folder, exist_ok=True)\n",
    "    \n",
    "    # Store the encrypted embedding in the corresponding folder\n",
    "    enc_embedding_file = os.path.join(encrypted_embedding_folder, os.path.basename(embedding_file) + \".txt\")\n",
    "    utilities.write_data(enc_embedding_file, enc_embedding_proto)\n",
    "    \n",
    "    # Clean up\n",
    "    del context, enc_embedding, enc_embedding_proto\n",
    "    \n",
    "    # Calculate the elapsed time for encryption\n",
    "    end_time = time.time()\n",
    "    encryption_time = end_time - start_time\n",
    "    \n",
    "    # Return the path to the encrypted embedding and the computation time\n",
    "    return enc_embedding_file, encryption_time\n",
    "\n",
    "# Specify the embedding and encrypted embedding folders\n",
    "embedding_folder = '/casia_Embeddings_246'\n",
    "encrypted_folder = '/casia_Encrypted-Embeddings_246/'\n",
    "\n",
    "# Select a single embedding file for testing (change this to the specific file path if needed)\n",
    "test_embedding_file = next(os.path.join(root, file) \n",
    "                           for root, _, files in os.walk(embedding_folder) for file in files)\n",
    "\n",
    "# Encrypt the single embedding and get the computation time\n",
    "enc_file, encryption_time = encrypt_embedding(test_embedding_file, embedding_folder, encrypted_folder)\n",
    "\n",
    "# Print the computation time for this single embedding encryption\n",
    "print(f\"Encrypted {os.path.basename(test_embedding_file)} in {encryption_time:.3f} seconds\")\n",
    "print(f\"Encrypted file saved to: {enc_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab557a2-035b-4c67-8d55-6a264cdd1dba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355be071-db35-47e2-bf70-9ae0701e847b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2925c93-2e98-4d94-bd7c-f692c4a53e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GEnuine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8303d0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the Euclidean distance between two encrypted embeddings\n",
    "def calculate_distance(enc_embedding_file1, enc_embedding_file2):\n",
    "    context = ts.context_from(utilities.read_data(\"/public_key.txt\"))\n",
    "    enc_embedding_proto1 = utilities.read_data(enc_embedding_file1)\n",
    "    enc_embedding_proto2 = utilities.read_data(enc_embedding_file2)\n",
    "    enc_embedding1 = ts.lazy_ckks_vector_from(enc_embedding_proto1)\n",
    "    enc_embedding1.link_context(context)\n",
    "    enc_embedding2 = ts.lazy_ckks_vector_from(enc_embedding_proto2)\n",
    "    enc_embedding2.link_context(context)\n",
    "    euclidean_squared = enc_embedding1 - enc_embedding2\n",
    "    euclidean_squared = euclidean_squared.dot(euclidean_squared)\n",
    "    euclidean_squared_file = os.path.join(euclidean_folder, os.path.basename(enc_embedding_file1) + \"_-_\" +\n",
    "                                          os.path.basename(enc_embedding_file2) + \".txt\")\n",
    "    utilities.write_data(euclidean_squared_file, euclidean_squared.serialize())\n",
    "    return euclidean_squared_file\n",
    "\n",
    "# Function to decrypt and compare the Euclidean distance\n",
    "def decrypt_and_compare(euclidean_squared_file):\n",
    "    if euclidean_squared_file is None:\n",
    "        return None  # Skip processing if the file path is None\n",
    "\n",
    "    context = ts.context_from(utilities.read_data(\"/secret_key.txt\"))\n",
    "    euclidean_squared_proto = utilities.read_data(euclidean_squared_file)\n",
    "    euclidean_squared = ts.lazy_ckks_vector_from(euclidean_squared_proto)\n",
    "    euclidean_squared.link_context(context)\n",
    "    euclidean_squared_plain = euclidean_squared.decrypt()[0]\n",
    "    return euclidean_squared_plain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c08e8ba-0b48-480b-8043-13c0187670bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genuine-Distance Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c512d2-c1be-4f55-8152-0176bd9661ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import tenseal as ts\n",
    "import utilities\n",
    "\n",
    "def calculate_distance(enc_embedding_file1, enc_embedding_file2):\n",
    "    context = ts.context_from(utilities.read_data(\"/public_key.txt\"))\n",
    "    enc_embedding_proto1 = utilities.read_data(enc_embedding_file1)\n",
    "    enc_embedding_proto2 = utilities.read_data(enc_embedding_file2)\n",
    "    enc_embedding1 = ts.lazy_ckks_vector_from(enc_embedding_proto1)\n",
    "    enc_embedding1.link_context(context)\n",
    "    enc_embedding2 = ts.lazy_ckks_vector_from(enc_embedding_proto2)\n",
    "    enc_embedding2.link_context(context)\n",
    "\n",
    "    # Calculate the squared Euclidean distance between the two encrypted vectors\n",
    "    euclidean_squared = enc_embedding1 - enc_embedding2\n",
    "    euclidean_squared = euclidean_squared.dot(euclidean_squared)\n",
    "\n",
    "    # Save the encrypted Euclidean distance result in a file\n",
    "    euclidean_squared_file = os.path.join(euclidean_folder, f\"{os.path.basename(enc_embedding_file1)}_-_{os.path.basename(enc_embedding_file2)}.txt\")\n",
    "    utilities.write_data(euclidean_squared_file, euclidean_squared.serialize())\n",
    "    \n",
    "    return euclidean_squared_file\n",
    "\n",
    "\n",
    "\n",
    "encrypted_folder = '/casia_Encrypted-Embeddings_246'\n",
    "euclidean_folder = '/casia_Genuine-Euclidean-Distances_246'\n",
    "\n",
    "\n",
    "# Create the Euclidean distance folder if it doesn't exist\n",
    "os.makedirs(euclidean_folder, exist_ok=True)\n",
    "\n",
    "# Get the list of class folders inside the encrypted embeddings folder\n",
    "class_folders = glob.glob(os.path.join(encrypted_folder, '*'))\n",
    "\n",
    "# Iterate over each class folder\n",
    "for class_folder in class_folders:\n",
    "    # Get the list of embedding files in the current class folder\n",
    "    embedding_files = glob.glob(os.path.join(class_folder, '*.txt'))\n",
    "\n",
    "    # Ensure that there are exactly 2 embeddings in each class folder for genuine distance calculation\n",
    "    if len(embedding_files) == 2:\n",
    "        # Calculate the Euclidean distance between the two embeddings\n",
    "        euclidean_squared_file = calculate_distance(embedding_files[0], embedding_files[1])\n",
    "\n",
    "        # Extract image names for logging\n",
    "        image1_name = os.path.splitext(os.path.basename(embedding_files[0]))[0]\n",
    "        image2_name = os.path.splitext(os.path.basename(embedding_files[1]))[0]\n",
    "\n",
    "        # Print the results to the console\n",
    "        print(f\"Class: {os.path.basename(class_folder)}, Image 1: {image1_name}, Image 2: {image2_name}, Euclidean Distance File: {euclidean_squared_file}\")\n",
    "\n",
    "print(\"Genuine distance calculation complete. Encrypted distances are stored in the specified folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe85aa42-d7fc-4131-aaae-292b6fb16526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decrypt the genuine distances and store them in a csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045e4ea1-0e58-4855-84c9-27e932111f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import tenseal as ts\n",
    "import utilities\n",
    "import csv\n",
    "\n",
    "def decrypt_and_generate_distances(euclidean_folder):\n",
    "    decrypted_distances = []\n",
    "    \n",
    "    # Loop through each encrypted file in the specified folder\n",
    "    for encrypted_file in glob.glob(os.path.join(euclidean_folder, '*.txt')):\n",
    "        try:\n",
    "            # Load the encryption context using the secret key\n",
    "            context = ts.context_from(utilities.read_data(\"/secret_key.txt\"))\n",
    "            \n",
    "            # Load the encrypted distance data\n",
    "            encrypted_distance_proto = utilities.read_data(encrypted_file)\n",
    "            encrypted_distance = ts.lazy_ckks_vector_from(encrypted_distance_proto)\n",
    "            encrypted_distance.link_context(context)\n",
    "            \n",
    "            # Decrypt the distance and extract the first value (scalar)\n",
    "            decrypted_distance = encrypted_distance.decrypt()[0]\n",
    "            \n",
    "            # Append the decrypted distance to the list\n",
    "            decrypted_distances.append(decrypted_distance)\n",
    "        except Exception as e:\n",
    "            # Handle decryption errors gracefully\n",
    "            print(f\"Error decrypting {encrypted_file}: {str(e)}\")\n",
    "\n",
    "    # Define the path for the output CSV file\n",
    "    csv_file = os.path.join(euclidean_folder, \"/casia_Genuine-Euclidean-Distances_246.csv\")\n",
    "    \n",
    "    # Write the decrypted distances to a CSV file\n",
    "    with open(csv_file, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Distance\"])  # CSV header\n",
    "        writer.writerows([[distance] for distance in decrypted_distances])  # Write each distance\n",
    "\n",
    "    print(f\"Decryption and distance generation completed. Total distances: {len(decrypted_distances)}\")\n",
    "\n",
    "# Set the folder containing the encrypted distance files\n",
    "euclidean_folder = '/casia_Genuine-Euclidean-Distances_246'\n",
    "\n",
    "# Decrypt and generate distances\n",
    "decrypt_and_generate_distances(euclidean_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb08e1f8-a6b7-49b4-bf9f-6702538920c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3a30f8-c87a-4859-8013-5ba2cfe40869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import tenseal as ts\n",
    "import utilities\n",
    "import csv\n",
    "\n",
    "def decrypt_and_generate_distances(euclidean_folder):\n",
    "    decrypted_distances = []\n",
    "    \n",
    "    # Loop through each encrypted file in the specified folder\n",
    "    for encrypted_file in glob.glob(os.path.join(euclidean_folder, '*.txt')):\n",
    "        try:\n",
    "            # Load the encryption context using the secret key\n",
    "            context = ts.context_from(utilities.read_data(\"/secret_key.txt\"))\n",
    "            \n",
    "            # Load the encrypted distance data\n",
    "            encrypted_distance_proto = utilities.read_data(encrypted_file)\n",
    "            encrypted_distance = ts.lazy_ckks_vector_from(encrypted_distance_proto)\n",
    "            encrypted_distance.link_context(context)\n",
    "            \n",
    "            # Decrypt the distance and extract the first value (scalar)\n",
    "            decrypted_distance = encrypted_distance.decrypt()[0]\n",
    "            \n",
    "            # Append the decrypted distance to the list\n",
    "            decrypted_distances.append(decrypted_distance)\n",
    "        except Exception as e:\n",
    "            # Handle decryption errors gracefully\n",
    "            print(f\"Error decrypting {encrypted_file}: {str(e)}\")\n",
    "\n",
    "    # Define the path for the output CSV file\n",
    "    \n",
    "    #csv_file = 'D:/Securing MULTIMODAL biometrics using FHE/FHE/Face-Genuine-Distances_without_Lpp.csv'\n",
    "    csv_file = 'D:/Securing MULTIMODAL biometrics using FHE/PAPER2 LATEST/FHE/casia_Genuine-Euclidean-Distances_246.csv'\n",
    "    \n",
    "    # Write the decrypted distances to a CSV file\n",
    "    with open(csv_file, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Distance\"])  # CSV header\n",
    "        writer.writerows([[distance] for distance in decrypted_distances])  # Write each distance\n",
    "\n",
    "    print(f\"Decryption and distance generation completed. Total distances: {len(decrypted_distances)}\")\n",
    "\n",
    "# Set the folder containing the encrypted distance files\n",
    "euclidean_folder = '/casia_Genuine-Euclidean-Distances_246'\n",
    "\n",
    "# Decrypt and generate distances\n",
    "decrypt_and_generate_distances(euclidean_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f354293-3a4d-4bf7-b5f0-c40972337f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMpostor Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d69a3ec-f80d-412e-92b7-7741e32066b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import tenseal as ts\n",
    "import utilities\n",
    "import csv\n",
    "\n",
    "def calculate_distance(enc_embedding_file1, enc_embedding_file2):\n",
    "    context = ts.context_from(utilities.read_data(\"D:/Securing MULTIMODAL biometrics using FHE/FHE/Keys/public_key.txt\"))\n",
    "    enc_embedding_proto1 = utilities.read_data(enc_embedding_file1)\n",
    "    enc_embedding_proto2 = utilities.read_data(enc_embedding_file2)\n",
    "    enc_embedding1 = ts.lazy_ckks_vector_from(enc_embedding_proto1)\n",
    "    enc_embedding1.link_context(context)\n",
    "    enc_embedding2 = ts.lazy_ckks_vector_from(enc_embedding_proto2)\n",
    "    enc_embedding2.link_context(context)\n",
    "    euclidean_squared = enc_embedding1 - enc_embedding2\n",
    "    euclidean_squared = euclidean_squared.dot(euclidean_squared)\n",
    "    euclidean_squared_file = os.path.join(impostor_folder, os.path.basename(enc_embedding_file1) + \"_-_\" +\n",
    "                                          os.path.basename(enc_embedding_file2) + \".txt\")\n",
    "    utilities.write_data(euclidean_squared_file, euclidean_squared.serialize())\n",
    "    return euclidean_squared_file\n",
    "\n",
    "def calculate_impostor_distances(embedding_folder):\n",
    "    # Get all class subfolders\n",
    "    class_folders = glob.glob(os.path.join(embedding_folder, '*'))\n",
    "    \n",
    "    # Iterate over each pair of different class folders\n",
    "    for i in range(len(class_folders)):\n",
    "        for j in range(i + 1, len(class_folders)):\n",
    "            class_folder1 = class_folders[i]\n",
    "            class_folder2 = class_folders[j]\n",
    "\n",
    "            # Get embedding files from each class folder\n",
    "            embedding_files1 = glob.glob(os.path.join(class_folder1, '*.txt'))\n",
    "            embedding_files2 = glob.glob(os.path.join(class_folder2, '*.txt'))\n",
    "\n",
    "            # Compare each embedding in class 1 with each embedding in class 2\n",
    "            for embedding_file1 in embedding_files1:\n",
    "                for embedding_file2 in embedding_files2:\n",
    "                    # Calculate and store the impostor distance\n",
    "                    impostor_distance_file = calculate_distance(embedding_file1, embedding_file2)\n",
    "\n",
    "                    # Log the result\n",
    "                    # print(f\"Class 1: {os.path.basename(class_folder1)}, Class 2: {os.path.basename(class_folder2)}, Distance File: {impostor_distance_file}\")\n",
    "\n",
    "# Set the embedding folder containing all class subfolders\n",
    "\n",
    "embedding_folder = '/casia_Encrypted-Embeddings_246/'\n",
    "\n",
    "# Set the folder to store impostor distances\n",
    "\n",
    "impostor_folder = '/casia_Impostor-Euclidean-Distances_246'\n",
    "os.makedirs(impostor_folder, exist_ok=True)\n",
    "\n",
    "# Calculate and store the impostor distances\n",
    "calculate_impostor_distances(embedding_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110b7874-831f-4a60-989c-086d65e3f1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import tenseal as ts\n",
    "import utilities\n",
    "import csv\n",
    "\n",
    "def decrypt_and_generate_distances(euclidean_folder):\n",
    "    decrypted_distances = []\n",
    "    \n",
    "    # Loop through each encrypted file in the specified folder\n",
    "    for encrypted_file in glob.glob(os.path.join(euclidean_folder, '*.txt')):\n",
    "        try:\n",
    "            # Load the encryption context using the secret key\n",
    "            context = ts.context_from(utilities.read_data(\"/secret_key.txt\"))\n",
    "            \n",
    "            # Load the encrypted distance data\n",
    "            encrypted_distance_proto = utilities.read_data(encrypted_file)\n",
    "            encrypted_distance = ts.lazy_ckks_vector_from(encrypted_distance_proto)\n",
    "            encrypted_distance.link_context(context)\n",
    "            \n",
    "            # Decrypt the distance and extract the first value (scalar)\n",
    "            decrypted_distance = encrypted_distance.decrypt()[0]\n",
    "            \n",
    "            # Append the decrypted distance to the list\n",
    "            decrypted_distances.append(decrypted_distance)\n",
    "        except Exception as e:\n",
    "            # Handle decryption errors gracefully\n",
    "            print(f\"Error decrypting {encrypted_file}: {str(e)}\")\n",
    "\n",
    "    # Define the path for the output CSV file\n",
    "    \n",
    "    csv_file = '/casia_Impostor-Euclidean-Distances_246.csv'\n",
    "    \n",
    "    # Write the decrypted distances to a CSV file\n",
    "    with open(csv_file, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Distance\"])  # CSV header\n",
    "        writer.writerows([[distance] for distance in decrypted_distances])  # Write each distance\n",
    "\n",
    "    print(f\"Decryption and distance generation completed. Total distances: {len(decrypted_distances)}\")\n",
    "\n",
    "# Set the folder containing the encrypted distance files\n",
    "#euclidean_folder = 'D:/Securing MULTIMODAL biometrics using FHE/FHE/Impostor-Euclidean-Distances'\n",
    "euclidean_folder = '/casia_Impostor-Euclidean-Distances_246'\n",
    "\n",
    "# Decrypt and generate distances\n",
    "decrypt_and_generate_distances(euclidean_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e17e9f-55d6-4e64-b312-a75893b944a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load distances from CSV files\n",
    "def load_distances(csv_filename):\n",
    "    distances = []\n",
    "    with open(csv_filename, 'r') as csvfile:\n",
    "        csv_reader = csv.reader(csvfile)\n",
    "        next(csv_reader)  # Skip header\n",
    "        for row in csv_reader:\n",
    "            dist = float(row[0])\n",
    "            distances.append(dist)\n",
    "    return distances\n",
    "\n",
    "# Calculate FAR, FRR, and EER\n",
    "def calculate_far_frr_eer(genuine_distances, impostor_distances):\n",
    "    genuine_distances = np.array(genuine_distances)\n",
    "    impostor_distances = np.array(impostor_distances)\n",
    "    \n",
    "    min_threshold = min(np.min(genuine_distances), np.min(impostor_distances))\n",
    "    max_threshold = max(np.max(genuine_distances), np.max(impostor_distances))\n",
    "    \n",
    "    thresholds = np.linspace(min_threshold, max_threshold, num=1000)\n",
    "    \n",
    "    frr = np.zeros(len(thresholds))\n",
    "    far = np.zeros(len(thresholds))\n",
    "\n",
    "    for i, threshold in enumerate(thresholds):\n",
    "        frr[i] = np.sum(genuine_distances > threshold) / len(genuine_distances)\n",
    "        far[i] = np.sum(impostor_distances <= threshold) / len(impostor_distances)\n",
    "        \n",
    "    eer_index = np.abs(frr - far).argmin()\n",
    "    eer = np.mean((frr[eer_index], far[eer_index]))\n",
    "    \n",
    "    return far, frr, eer, thresholds[eer_index], thresholds\n",
    "\n",
    "# File paths\n",
    "\n",
    "\n",
    "genuine_output_filename = '/casia_Genuine-Euclidean-Distances_246.csv'\n",
    "impostor_output_filename = '/casia_Impostor-Euclidean-Distances_246.csv'\n",
    "\n",
    "\n",
    "# Load distances from CSV files\n",
    "genuine_distances = load_distances(genuine_output_filename)\n",
    "impostor_distances = load_distances(impostor_output_filename)\n",
    "\n",
    "# Calculate FAR, FRR, and EER\n",
    "far, frr, eer, eer_threshold, thresholds = calculate_far_frr_eer(genuine_distances, impostor_distances)\n",
    "\n",
    "# Print results\n",
    "print(f\"EER: {eer}\")\n",
    "print(f\"Threshold at EER: {eer_threshold}\")\n",
    "\n",
    "# Plot FAR and FRR\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, far, label='FAR', color='red')\n",
    "plt.plot(thresholds, frr, label='FRR', color='blue')\n",
    "plt.axvline(x=eer_threshold, linestyle='--', color='green', label=f'EER Threshold: {eer_threshold:.4f}')\n",
    "plt.axhline(y=eer, linestyle='--', color='purple', label=f'EER: {eer:.4f}')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Rate')\n",
    "plt.title('FAR and FRR vs. Threshold')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Save the plot\n",
    "\n",
    "output_plot_filename = '/casia_Facenet_euclidean_EER_246_FHE.png'\n",
    "plt.savefig(output_plot_filename)\n",
    "\n",
    "print(f\"Plot saved as {output_plot_filename}\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbbc7e7-395b-4c26-87d8-6523e4eb1818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c283a6dd-f8d3-45ea-b5c2-05f07a463f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_confusion_matrix(genuine_distances, impostor_distances, threshold):\n",
    "    # Convert lists to NumPy arrays\n",
    "    genuine_distances = np.array(genuine_distances)\n",
    "    impostor_distances = np.array(impostor_distances)\n",
    "    \n",
    "    tp = np.sum(genuine_distances <= threshold)  # True Positives\n",
    "    fn = np.sum(genuine_distances > threshold)   # False Negatives\n",
    "    tn = np.sum(impostor_distances > threshold)  # True Negatives\n",
    "    fp = np.sum(impostor_distances <= threshold) # False Positives\n",
    "    \n",
    "    return tp, tn, fp, fn\n",
    "\n",
    "def calculate_accuracy(tp, tn, fp, fn):\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    return accuracy\n",
    "\n",
    "# Assuming you've already calculated the threshold at EER using the existing code\n",
    "eer_threshold = -5.598\n",
    " \n",
    "\n",
    "# Call the confusion matrix function\n",
    "tp, tn, fp, fn = calculate_confusion_matrix(genuine_distances, impostor_distances, eer_threshold)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = calculate_accuracy(tp, tn, fp, fn)\n",
    "\n",
    "# Print results\n",
    "print(f\"True Positives (TP): {tp}\")\n",
    "print(f\"True Negatives (TN): {tn}\")\n",
    "print(f\"False Positives (FP): {fp}\")\n",
    "print(f\"False Negatives (FN): {fn}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc513ed-abac-4d8b-bc2e-443a81bf26d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision(tp, fp):\n",
    "    return tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "\n",
    "def calculate_recall(tp, fn):\n",
    "    return tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "def calculate_f1_score(precision, recall):\n",
    "    return 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "# Call the confusion matrix function\n",
    "tp, tn, fp, fn = calculate_confusion_matrix(genuine_distances, impostor_distances, eer_threshold)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = calculate_accuracy(tp, tn, fp, fn)\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision = calculate_precision(tp, fp)\n",
    "recall = calculate_recall(tp, fn)\n",
    "f1_score = calculate_f1_score(precision, recall)\n",
    "\n",
    "# Print results\n",
    "print(f\"True Positives (TP): {tp}\")\n",
    "print(f\"True Negatives (TN): {tn}\")\n",
    "print(f\"False Positives (FP): {fp}\")\n",
    "print(f\"False Negatives (FN): {fn}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6db575-1152-432f-b93c-0bf7182712ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83f5a8c-6525-45ff-a561-22297afa6afc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4b7bd9-9f69-4c15-97a9-c18f573de4ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a1fe29-a757-4ad4-a967-e54de19dc599",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9079aefa-e162-4d0b-bc35-d8cf2f1f18f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d92269-f85a-49fe-a9ce-96b4a6452a25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d95ae4-9b11-4ca4-aec6-86ecfe7d32bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726f5f33-e0fc-4982-9df8-2385f6b8abe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae02c5c7-4a7a-4635-93a4-3e50b9216afe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55833cc5-715d-45a8-9e3c-c969ffecbb1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff420e4-a9a1-4032-a6ac-5846f36cea85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b243ff1-54cb-48a8-8126-6927932552cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8798ff3-c20e-4f6e-96ef-c40cc88d3aea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ecba38-1c61-4594-a9b2-164f9ed8821c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbbe323-26a9-4eb3-a704-9c4e0d3f049e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
